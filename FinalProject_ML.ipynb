{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe's Style Analyis with ML Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment I will try to replicate Sharpe's Style Analysis of an investment fund with data from various MSCI Indexes and EONIA.\n",
    "In this first code snippet I will simply filter the daily navs and indexes levels for a set of specific dates, which is the weekly dates for return calculation.\n",
    "\n",
    "Everything is described by comments in the actual code snippet.\n",
    "\n",
    "NB: I WOULD NOT RE-RUN THE SCRIPT AS SOME OF THE GRID SEARCHES ARE NOT THAT FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fund and indexes return\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# As it will happens that some models will not covnerge, I will ignore the message\n",
    "# for the sake of a clear output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "INDEXES_DATA = '/Users/marco/Desktop/Tesi MSc/AppData/Indexes_quotations.csv'\n",
    "FUNDS_NAME_DATA = '/Users/marco/Desktop/Tesi MSc/AppData/Funds_name.csv'\n",
    "FUNDS_NAV_DATA = '/Users/marco/Desktop/Tesi MSc/AppData/Funds_navc.csv'\n",
    "WEEKLY_DATES_DATA = '/Users/marco/Desktop/Tesi MSc/Weekly_dates.csv' \n",
    "\n",
    "\n",
    "# load all data\n",
    "#\n",
    "funds_name_df = pd.read_csv(FUNDS_NAME_DATA, header=0, engine='python')\n",
    "indexes_level = pd.read_csv(INDEXES_DATA, header=0, engine='python')\n",
    "funds_nav = pd.read_csv(FUNDS_NAV_DATA, header=0, engine='python').iloc[:, 1:]\n",
    "all_dates_df = pd.read_csv(WEEKLY_DATES_DATA, header=0, engine='python').iloc[:, 0:1]\n",
    "\n",
    "correct_dates_for_nav = indexes_level.iloc[:, 0:1]\n",
    "funds_nav = pd.concat([correct_dates_for_nav, funds_nav], axis=1)\n",
    "# put dates into list for filtering\n",
    "#\n",
    "all_dates = list(all_dates_df['Dates'])\n",
    "\n",
    "# filter indexes for weely return calculation (friday to friday)\n",
    "indexes_weekly = indexes_level[indexes_level['Dates'].isin(all_dates)].reset_index(drop=True)\n",
    "return_indexes_weekly = indexes_weekly.iloc[:, 1:].pct_change().iloc[1:, :]\n",
    "\n",
    "# divide into strategic and geographic, but I will carry the analysis only on strategic as I think geographic\n",
    "# is not really meaningful and the approach would be exactly the same, also eonia is needed\n",
    "eonia = return_indexes_weekly.iloc[:, 0:1]\n",
    "strategic_return = return_indexes_weekly.iloc[:, 10:21]\n",
    "strategic_return = pd.concat([eonia, strategic_return], axis=1)\n",
    "\n",
    "# filter nav_return and select the specifi fund, in this case I will perform the analysis only on the \n",
    "# first one which will be a Blackrock fund, but this can be extended to other fund as well, so I will calculate\n",
    "# all returns but use only the first column\n",
    "funds_nav = funds_nav[funds_nav['Dates'].isin(all_dates)].reset_index(drop=True)\n",
    "return_funds = funds_nav.iloc[:, 1:].pct_change().iloc[1:, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT0000677927</th>\n",
       "      <th>AT0000712575</th>\n",
       "      <th>AT0000785266</th>\n",
       "      <th>AT0000A09ZL0</th>\n",
       "      <th>BE0058652646</th>\n",
       "      <th>BE0946564383</th>\n",
       "      <th>BE0946893766</th>\n",
       "      <th>BE0947574787</th>\n",
       "      <th>BE6228801435</th>\n",
       "      <th>DE0008474024</th>\n",
       "      <th>...</th>\n",
       "      <th>NL0009690221</th>\n",
       "      <th>NL0010408704</th>\n",
       "      <th>IE00B5340Q90</th>\n",
       "      <th>LU0998532633</th>\n",
       "      <th>LU1299311834</th>\n",
       "      <th>LU1242470554</th>\n",
       "      <th>FR0011108331</th>\n",
       "      <th>LU0342679015</th>\n",
       "      <th>LU0973154932</th>\n",
       "      <th>GB00BYRJNL93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.017733</td>\n",
       "      <td>-0.033651</td>\n",
       "      <td>-0.020138</td>\n",
       "      <td>-0.031269</td>\n",
       "      <td>-0.042595</td>\n",
       "      <td>-0.049802</td>\n",
       "      <td>-0.047938</td>\n",
       "      <td>-0.066993</td>\n",
       "      <td>-0.042887</td>\n",
       "      <td>-0.014313</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039902</td>\n",
       "      <td>-0.043665</td>\n",
       "      <td>-0.011011</td>\n",
       "      <td>-0.047178</td>\n",
       "      <td>-0.025419</td>\n",
       "      <td>-0.012731</td>\n",
       "      <td>-0.030272</td>\n",
       "      <td>-0.012842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.026288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005818</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>-0.005200</td>\n",
       "      <td>0.017959</td>\n",
       "      <td>0.010615</td>\n",
       "      <td>0.010148</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.012343</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007970</td>\n",
       "      <td>0.008669</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.011067</td>\n",
       "      <td>0.012465</td>\n",
       "      <td>-0.021291</td>\n",
       "      <td>0.016170</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015737</td>\n",
       "      <td>-0.026947</td>\n",
       "      <td>-0.011774</td>\n",
       "      <td>-0.014216</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.014442</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>-0.005103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013273</td>\n",
       "      <td>0.012033</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.015192</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>0.021040</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>-0.009503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.027050</td>\n",
       "      <td>0.022473</td>\n",
       "      <td>0.023790</td>\n",
       "      <td>0.019982</td>\n",
       "      <td>-0.004030</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>-0.006838</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.016730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.068873</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.015237</td>\n",
       "      <td>-0.029940</td>\n",
       "      <td>0.001059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.061175</td>\n",
       "      <td>-0.034485</td>\n",
       "      <td>-0.060069</td>\n",
       "      <td>-0.057377</td>\n",
       "      <td>-0.072570</td>\n",
       "      <td>-0.066020</td>\n",
       "      <td>-0.065250</td>\n",
       "      <td>-0.078432</td>\n",
       "      <td>-0.062585</td>\n",
       "      <td>-0.054421</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065907</td>\n",
       "      <td>-0.067871</td>\n",
       "      <td>-0.019038</td>\n",
       "      <td>-0.001298</td>\n",
       "      <td>-0.032055</td>\n",
       "      <td>-0.028346</td>\n",
       "      <td>-0.028390</td>\n",
       "      <td>-0.046693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.049735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>-0.029794</td>\n",
       "      <td>-0.029514</td>\n",
       "      <td>-0.023757</td>\n",
       "      <td>-0.043437</td>\n",
       "      <td>-0.036835</td>\n",
       "      <td>-0.034205</td>\n",
       "      <td>-0.038897</td>\n",
       "      <td>-0.018516</td>\n",
       "      <td>-0.041691</td>\n",
       "      <td>-0.031411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041798</td>\n",
       "      <td>-0.039263</td>\n",
       "      <td>-0.010537</td>\n",
       "      <td>-0.040107</td>\n",
       "      <td>-0.031937</td>\n",
       "      <td>-0.038810</td>\n",
       "      <td>-0.027875</td>\n",
       "      <td>-0.044437</td>\n",
       "      <td>-0.000175</td>\n",
       "      <td>-0.042355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.039808</td>\n",
       "      <td>0.037769</td>\n",
       "      <td>0.048546</td>\n",
       "      <td>0.040602</td>\n",
       "      <td>0.066214</td>\n",
       "      <td>0.068536</td>\n",
       "      <td>0.051344</td>\n",
       "      <td>0.060983</td>\n",
       "      <td>0.042123</td>\n",
       "      <td>0.052290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057824</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>0.011715</td>\n",
       "      <td>0.065536</td>\n",
       "      <td>0.080854</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>0.038364</td>\n",
       "      <td>0.060971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.022855</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>-0.002191</td>\n",
       "      <td>0.031337</td>\n",
       "      <td>0.009160</td>\n",
       "      <td>-0.003046</td>\n",
       "      <td>0.027866</td>\n",
       "      <td>0.028774</td>\n",
       "      <td>0.061736</td>\n",
       "      <td>0.016581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046991</td>\n",
       "      <td>0.037774</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.023353</td>\n",
       "      <td>0.014859</td>\n",
       "      <td>0.076613</td>\n",
       "      <td>0.010362</td>\n",
       "      <td>0.020455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.012776</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.026931</td>\n",
       "      <td>-0.010383</td>\n",
       "      <td>-0.001528</td>\n",
       "      <td>0.006987</td>\n",
       "      <td>0.012782</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.032924</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>-0.003818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.015241</td>\n",
       "      <td>0.010137</td>\n",
       "      <td>0.024753</td>\n",
       "      <td>0.005870</td>\n",
       "      <td>0.008279</td>\n",
       "      <td>0.018873</td>\n",
       "      <td>-0.004987</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>0.019680</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>0.018777</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>0.006598</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 320 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AT0000677927  AT0000712575  AT0000785266  AT0000A09ZL0  BE0058652646  \\\n",
       "0       -0.017733     -0.033651     -0.020138     -0.031269     -0.042595   \n",
       "1        0.005818      0.014945      0.001572     -0.005200      0.017959   \n",
       "2       -0.015737     -0.026947     -0.011774     -0.014216      0.011494   \n",
       "3        0.027050      0.022473      0.023790      0.019982     -0.004030   \n",
       "4       -0.061175     -0.034485     -0.060069     -0.057377     -0.072570   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "255     -0.029794     -0.029514     -0.023757     -0.043437     -0.036835   \n",
       "256      0.039808      0.037769      0.048546      0.040602      0.066214   \n",
       "257      0.022855      0.024097     -0.002191      0.031337      0.009160   \n",
       "258      0.012776      0.000865      0.006764      0.026931     -0.010383   \n",
       "259      0.005502      0.015241      0.010137      0.024753      0.005870   \n",
       "\n",
       "     BE0946564383  BE0946893766  BE0947574787  BE6228801435  DE0008474024  \\\n",
       "0       -0.049802     -0.047938     -0.066993     -0.042887     -0.014313   \n",
       "1        0.010615      0.010148      0.029500      0.012343      0.006682   \n",
       "2        0.014442      0.017002      0.019361      0.010285     -0.005103   \n",
       "3        0.001202      0.001520     -0.006838      0.003432      0.016730   \n",
       "4       -0.066020     -0.065250     -0.078432     -0.062585     -0.054421   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "255     -0.034205     -0.038897     -0.018516     -0.041691     -0.031411   \n",
       "256      0.068536      0.051344      0.060983      0.042123      0.052290   \n",
       "257     -0.003046      0.027866      0.028774      0.061736      0.016581   \n",
       "258     -0.001528      0.006987      0.012782      0.004811      0.004680   \n",
       "259      0.008279      0.018873     -0.004987      0.017402      0.005036   \n",
       "\n",
       "     ...  NL0009690221  NL0010408704  IE00B5340Q90  LU0998532633  \\\n",
       "0    ...     -0.039902     -0.043665     -0.011011     -0.047178   \n",
       "1    ...      0.007970      0.008669      0.006073      0.011067   \n",
       "2    ...      0.013273      0.012033      0.004024      0.015192   \n",
       "3    ...      0.002230      0.003114      0.000000     -0.068873   \n",
       "4    ...     -0.065907     -0.067871     -0.019038     -0.001298   \n",
       "..   ...           ...           ...           ...           ...   \n",
       "255  ...     -0.041798     -0.039263     -0.010537     -0.040107   \n",
       "256  ...      0.057824      0.059684      0.011715      0.065536   \n",
       "257  ...      0.046991      0.037774      0.004211      0.023353   \n",
       "258  ...      0.009847      0.013405      0.000000      0.001769   \n",
       "259  ...      0.020862      0.019680      0.002096      0.005847   \n",
       "\n",
       "     LU1299311834  LU1242470554  FR0011108331  LU0342679015  LU0973154932  \\\n",
       "0       -0.025419     -0.012731     -0.030272     -0.012842      0.000000   \n",
       "1        0.012465     -0.021291      0.016170      0.011878      0.000000   \n",
       "2        0.010759      0.021040      0.007680     -0.009503      0.000000   \n",
       "3        0.005834      0.002201      0.000793      0.015237     -0.029940   \n",
       "4       -0.032055     -0.028346     -0.028390     -0.046693      0.000000   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "255     -0.031937     -0.038810     -0.027875     -0.044437     -0.000175   \n",
       "256      0.080854      0.055168      0.038364      0.060971      0.000000   \n",
       "257      0.014859      0.076613      0.010362      0.020455      0.000000   \n",
       "258      0.002524      0.032924      0.001020     -0.003818      0.000000   \n",
       "259      0.018777      0.021298      0.006598      0.000958      0.000000   \n",
       "\n",
       "     GB00BYRJNL93  \n",
       "0       -0.026288  \n",
       "1       -0.002160  \n",
       "2        0.021645  \n",
       "3        0.001059  \n",
       "4       -0.049735  \n",
       "..            ...  \n",
       "255     -0.042355  \n",
       "256      0.047465  \n",
       "257      0.050463  \n",
       "258      0.027451  \n",
       "259      0.022901  \n",
       "\n",
       "[260 rows x 320 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_funds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EONIA</th>\n",
       "      <th>MSCI World/Consumer Discrionary</th>\n",
       "      <th>MSCI World/Consumer Staples</th>\n",
       "      <th>MSCI World/Energy</th>\n",
       "      <th>MSCI World/Financials</th>\n",
       "      <th>MSCI World/Health Care</th>\n",
       "      <th>MSCI World/Industrials</th>\n",
       "      <th>MSCI World/Information Tech</th>\n",
       "      <th>MSCI World/Materials</th>\n",
       "      <th>MSCI World/Telecom svc</th>\n",
       "      <th>MSCI World/Utilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.036289</td>\n",
       "      <td>-0.024476</td>\n",
       "      <td>-0.069547</td>\n",
       "      <td>-0.048661</td>\n",
       "      <td>-0.022309</td>\n",
       "      <td>-0.031703</td>\n",
       "      <td>-0.041175</td>\n",
       "      <td>-0.041774</td>\n",
       "      <td>-0.027609</td>\n",
       "      <td>-0.018963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.013142</td>\n",
       "      <td>-0.001813</td>\n",
       "      <td>0.014226</td>\n",
       "      <td>0.020149</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.003856</td>\n",
       "      <td>-0.008806</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.023459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.014645</td>\n",
       "      <td>0.039213</td>\n",
       "      <td>0.013183</td>\n",
       "      <td>0.012922</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>0.030722</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>0.009999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.019262</td>\n",
       "      <td>-0.000756</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>-0.007558</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.004284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.062126</td>\n",
       "      <td>-0.035926</td>\n",
       "      <td>-0.073853</td>\n",
       "      <td>-0.070861</td>\n",
       "      <td>-0.051211</td>\n",
       "      <td>-0.058149</td>\n",
       "      <td>-0.066517</td>\n",
       "      <td>-0.074014</td>\n",
       "      <td>-0.024531</td>\n",
       "      <td>-0.015161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.045958</td>\n",
       "      <td>-0.035880</td>\n",
       "      <td>-0.046455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041954</td>\n",
       "      <td>-0.048394</td>\n",
       "      <td>-0.055245</td>\n",
       "      <td>-0.038335</td>\n",
       "      <td>-0.024790</td>\n",
       "      <td>-0.032669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.066188</td>\n",
       "      <td>0.038602</td>\n",
       "      <td>0.016144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.067712</td>\n",
       "      <td>0.082704</td>\n",
       "      <td>0.062207</td>\n",
       "      <td>0.056239</td>\n",
       "      <td>0.031073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.037380</td>\n",
       "      <td>0.155923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017144</td>\n",
       "      <td>0.048730</td>\n",
       "      <td>-0.001249</td>\n",
       "      <td>0.014422</td>\n",
       "      <td>0.013702</td>\n",
       "      <td>0.036516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.015188</td>\n",
       "      <td>-0.013629</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.025324</td>\n",
       "      <td>0.012304</td>\n",
       "      <td>-0.003947</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>-0.005813</td>\n",
       "      <td>-0.022369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.031423</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.078451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>0.013079</td>\n",
       "      <td>0.019541</td>\n",
       "      <td>0.025435</td>\n",
       "      <td>0.023387</td>\n",
       "      <td>0.002502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        EONIA  MSCI World/Consumer Discrionary  MSCI World/Consumer Staples  \\\n",
       "1   -0.000036                        -0.036289                    -0.024476   \n",
       "2   -0.000053                         0.006481                     0.013142   \n",
       "3   -0.000053                         0.003628                     0.014645   \n",
       "4   -0.000036                         0.003705                    -0.000262   \n",
       "5   -0.000036                        -0.062126                    -0.035926   \n",
       "..        ...                              ...                          ...   \n",
       "256 -0.000091                        -0.045958                    -0.035880   \n",
       "257 -0.000091                         0.066188                     0.038602   \n",
       "258 -0.000091                         0.006421                     0.037380   \n",
       "259 -0.000091                         0.015188                    -0.013629   \n",
       "260 -0.000091                         0.031423                     0.000801   \n",
       "\n",
       "     MSCI World/Energy  MSCI World/Financials  MSCI World/Health Care  \\\n",
       "1            -0.069547              -0.048661               -0.022309   \n",
       "2            -0.001813               0.014226                0.020149   \n",
       "3             0.039213               0.013183                0.012922   \n",
       "4            -0.019262              -0.000756                0.001938   \n",
       "5            -0.073853              -0.070861               -0.051211   \n",
       "..                 ...                    ...                     ...   \n",
       "256          -0.046455               0.000000               -0.041954   \n",
       "257           0.016144               0.000000                0.071700   \n",
       "258           0.155923               0.000000                0.017144   \n",
       "259           0.050417               0.000000               -0.025324   \n",
       "260           0.078451               0.000000                0.004030   \n",
       "\n",
       "     MSCI World/Industrials  MSCI World/Information Tech  \\\n",
       "1                 -0.031703                    -0.041175   \n",
       "2                  0.000655                    -0.003856   \n",
       "3                  0.012564                     0.011945   \n",
       "4                  0.003231                    -0.000659   \n",
       "5                 -0.058149                    -0.066517   \n",
       "..                      ...                          ...   \n",
       "256               -0.048394                    -0.055245   \n",
       "257                0.067712                     0.082704   \n",
       "258                0.048730                    -0.001249   \n",
       "259                0.012304                    -0.003947   \n",
       "260                0.013079                     0.019541   \n",
       "\n",
       "     MSCI World/Materials  MSCI World/Telecom svc  MSCI World/Utilities  \n",
       "1               -0.041774               -0.027609             -0.018963  \n",
       "2               -0.008806                0.016380              0.023459  \n",
       "3                0.030722                0.013088              0.009999  \n",
       "4               -0.007558                0.002218              0.004284  \n",
       "5               -0.074014               -0.024531             -0.015161  \n",
       "..                    ...                     ...                   ...  \n",
       "256             -0.038335               -0.024790             -0.032669  \n",
       "257              0.062207                0.056239              0.031073  \n",
       "258              0.014422                0.013702              0.036516  \n",
       "259              0.007750               -0.005813             -0.022369  \n",
       "260              0.025435                0.023387              0.002502  \n",
       "\n",
       "[260 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategic_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sCodeISIN</th>\n",
       "      <th>Nom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LU0238689110</td>\n",
       "      <td>BlackRock Global Dynamic Equity Fund A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LU0265550359</td>\n",
       "      <td>BlackRock Systmatc Glb Eq Hgh Inc A2 USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LU0217139020</td>\n",
       "      <td>Pictet Premium Brands P EUR Acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LU0101692670</td>\n",
       "      <td>Pictet Digital P USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LU0097427784</td>\n",
       "      <td>JSS Sust Equity Global P EUR Dist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sCodeISIN                                       Nom\n",
       "0  LU0238689110   BlackRock Global Dynamic Equity Fund A2\n",
       "1  LU0265550359  BlackRock Systmatc Glb Eq Hgh Inc A2 USD\n",
       "2  LU0217139020           Pictet Premium Brands P EUR Acc\n",
       "3  LU0101692670                      Pictet Digital P USD\n",
       "4  LU0097427784         JSS Sust Equity Global P EUR Dist"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funds_name_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now data are ready to be used, my intent is to try to replicate Sharpe's Style Analysis with the Machine Learning tools we have learned throughout the course. \n",
    "The idea is to regress the various indexes to replicate as much as possible one of the fund. As performance metrics we will use the classic r^2 as the problem is actually a regression.\n",
    "\n",
    "Nevertheless this r^2 can be interpreted as the \"percentage activity\" of the fund, the lower the r^2 the higher the fund activity and thus it's more difficult to replicate its performance using a dynamic combination of passive indexes (EONIA+MSCI).\n",
    "On the other side, if the r^2 is high it will imply that the fund is actively trading on the market and thus deserve to be paid an higher fee.\n",
    "\n",
    "This is a really simplified and reductive approach to Sharpe's Style Analysis, but to explain this concept is not really the scope of this project. This simple introduction was to say that if our model performs poorly in terms of r^2 is not necessarily due to a problem in the model but should actually be a point in favour of the fund manager.\n",
    "\n",
    "For this analysis we will use the first fund in the funds_name_df which is BlackRock Global Dynamic Equity Fund A2 and its ISIN code is LU0238689110, we will use this code to extract the returns of the selected fund from the dataframe that contains all funds return. As I have calculated the r^2 with the traditional style analysis on an different Excel file for other purposes, for the last year of available data the r^2 should be of 86.46%.\n",
    "So this will be our benchmark for the result and I will split the data so that our test sample will be the last year, so 52 observations out of the 260 available. This implies that our training set is only 208 observations, which is not really sufficient for a good training, but is still a relevant size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Regression\n",
    "\n",
    "The first step of our assignment will be to estimate the results with a simple regression to have an idea of how a standard model would perform in terms of R^2 for this datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full sample R-squared:\n",
      "0.8478228685655933\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "# Run on the full sample\n",
    "X = strategic_return\n",
    "y = return_funds['LU0238689110']\n",
    "lin_reg.fit(X,y)\n",
    "print(\"Full sample R-squared:\")\n",
    "print(lin_reg.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.8801336985713125\n",
      "0.49022502272283797\n",
      "Intercept/coefficient:\n",
      "[-0.0035614636926175314, array([-4.40670781e+01,  1.85945062e-01,  1.65735689e-01,  3.04002447e-02,\n",
      "        9.80686955e-02,  1.32073924e-01,  9.09889392e-02,  8.35128641e-03,\n",
      "        2.03056577e-01,  1.29819158e-01, -5.14252504e-02])]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=52)\n",
    "lin_reg.fit(X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(lin_reg.score(X_train,y_train))\n",
    "print(lin_reg.score(X_test,y_test))\n",
    "print(\"Intercept/coefficient:\")\n",
    "print([lin_reg.intercept_,lin_reg.coef_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "In this part we will train and test a Ridge model, in this situation we prefer a L2 regularization approach as my intetn is not to shut various parameter to zero, rather to simply decrese their weights in case they are not really relevant for the selected period. This is the main reason I decided to carry out only a Ridge regressiin and not a Lasso too.\n",
    "\n",
    "For a Machine Learning project is important to fine-tuning the hyperparameters selected, as a consequence I will perform a Grid Search on all the models used, in order to select the best set of hyperparameters to then train the best model. This methodology will be adopted for all other models.\n",
    "\n",
    "It is also important to point out that, as we cannot perform the standard cross validation, as we would lose the structure of time series data, we use the TimeSeriesSplit available in scikit which is reccomanded for Grid Searches in these situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank_test_score  mean_test_score param_alpha\n",
      "0                50         0.819138      0.0005\n",
      "1                35         0.834146      0.0015\n",
      "2                27         0.840131      0.0025\n",
      "3                21         0.843516      0.0035\n",
      "4                16         0.845687      0.0045\n",
      "5                13         0.847148      0.0055\n",
      "6                10         0.848139      0.0065\n",
      "7                 7         0.848796      0.0075\n",
      "8                 5         0.849202      0.0085\n",
      "9                 2         0.849414      0.0095\n",
      "10                1         0.849472      0.0105\n",
      "11                3         0.849405      0.0115\n",
      "12                4         0.849235      0.0125\n",
      "13                6         0.848980      0.0135\n",
      "14                8         0.848652      0.0145\n",
      "15                9         0.848264      0.0155\n",
      "16               11         0.847823      0.0165\n",
      "17               12         0.847337      0.0175\n",
      "18               14         0.846811      0.0185\n",
      "19               15         0.846252      0.0195\n",
      "20               17         0.845663      0.0205\n",
      "21               18         0.845047      0.0215\n",
      "22               19         0.844408      0.0225\n",
      "23               20         0.843748      0.0235\n",
      "24               22         0.843070      0.0245\n",
      "25               23         0.842375      0.0255\n",
      "26               24         0.841666      0.0265\n",
      "27               25         0.840943      0.0275\n",
      "28               26         0.840209      0.0285\n",
      "29               28         0.839464      0.0295\n",
      "30               29         0.838709      0.0305\n",
      "31               30         0.837946      0.0315\n",
      "32               31         0.837174      0.0325\n",
      "33               32         0.836396      0.0335\n",
      "34               33         0.835611      0.0345\n",
      "35               34         0.834820      0.0355\n",
      "36               36         0.834024      0.0365\n",
      "37               37         0.833223      0.0375\n",
      "38               38         0.832418      0.0385\n",
      "39               39         0.831608      0.0395\n",
      "40               40         0.830795      0.0405\n",
      "41               41         0.829979      0.0415\n",
      "42               42         0.829160      0.0425\n",
      "43               43         0.828338      0.0435\n",
      "44               44         0.827514      0.0445\n",
      "45               45         0.826688      0.0455\n",
      "46               46         0.825860      0.0465\n",
      "47               47         0.825030      0.0475\n",
      "48               48         0.824198      0.0485\n",
      "49               49         0.823366      0.0495\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Best result: \n",
      "{'alpha': 0.0105}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "param_grid={'alpha':np.arange(start=0.0005, stop=0.05,step=0.001)}\n",
    "ridge = Ridge()\n",
    "grid_search=GridSearchCV(ridge,param_grid,scoring='r2', cv=tscv, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(results[['rank_test_score','mean_test_score','param_alpha']])\n",
    "print(\"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"\\n\")\n",
    "print('Best result: ')\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.8791605618284779\n",
      "0.45872747429281346\n"
     ]
    }
   ],
   "source": [
    "# train the best result of the grid search so alpha = 0.05\n",
    "best_ridge = Ridge(alpha=0.0055)\n",
    "best_ridge.fit(X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(best_ridge.score(X_train,y_train))\n",
    "print(best_ridge.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of the grid search and then the results of the fine-tuned ridge (alpha=0.0055) we can see that the model does not perform as good as expected in the test sample. We are close to the 86% stated at the begininng but it's still in an acceptable range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Near Neighbours (KNN)\n",
    "\n",
    "The next model tested is the KNN, which is really different from out initial objective. Here we don't have any resemblance of a functional form, which should be the a linear relationship. As the KNN only have one parameters to search for, we will concentrate on the number of neighbours to maximize our fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank_test_score  mean_test_score param_n_neighbors\n",
      "0                 6         0.704257                 2\n",
      "1                 4         0.709061                 3\n",
      "2                 1         0.727624                 4\n",
      "3                 2         0.723432                 5\n",
      "4                 3         0.710300                 6\n",
      "5                 5         0.708336                 7\n",
      "6                 7         0.694908                 8\n",
      "7                 8         0.682052                 9\n",
      "8                 9         0.676380                10\n",
      "9                10         0.668074                11\n",
      "10               11         0.662954                12\n",
      "11               12         0.648442                13\n",
      "12               13         0.638571                14\n",
      "13               14         0.628275                15\n",
      "14               15         0.615135                16\n",
      "15               16         0.605471                17\n",
      "16               17         0.595254                18\n",
      "17               18         0.584372                19\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Best result: \n",
      "{'n_neighbors': 4}\n"
     ]
    }
   ],
   "source": [
    "# k-neighobours regression\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "pg_knnreg = {'n_neighbors' : np.arange(start=2, stop=20,step=1)}\n",
    "knnreg = reg = KNeighborsRegressor()\n",
    "grids_knnreg = GridSearchCV(knnreg,pg_knnreg,scoring='r2', cv=tscv, return_train_score=True)\n",
    "grids_knnreg.fit(X_train,y_train)\n",
    "results_knnreg = pd.DataFrame(grids_knnreg.cv_results_)\n",
    "print(results_knnreg[['rank_test_score','mean_test_score','param_n_neighbors']])\n",
    "print(\"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"\\n\")\n",
    "print('Best result: ')\n",
    "print(grids_knnreg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.9028203836429722\n",
      "0.6756473596523109\n"
     ]
    }
   ],
   "source": [
    "knn_bestmodel = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_bestmodel.fit(X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(knn_bestmodel.score(X_train,y_train))\n",
    "print(knn_bestmodel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is really similar to a Ridge regression even if the underlying model is completely different. Nevertheless even this model does not providing better results than what our basic model yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machines \n",
    "\n",
    "Here we come back to a model with a functional form, which reasonably seems to fit better to what we are looking for. Furthermore this is a linear model, which theoretically should be really similar to what we are looking for.\n",
    "As in SVM we have more hyperparameters to fine-tune, here we set the dictionary with epsilon and max_iter as the parameters to grid search for.\n",
    "\n",
    "As we can see in the output, during the training of so many models, the SVM might not converge (I kept 10000 as max_iter to avoid making the script too long to run), in this case the output reports a warning in the begininng, which we will simply ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank_test_score  mean_test_score param_epsilon\n",
      "0                 1         0.890018         1e-05\n",
      "1                 2         0.889916         6e-05\n",
      "2                 3         0.889847       0.00011\n",
      "3                 4         0.889685       0.00016\n",
      "4                 5         0.888803       0.00021\n",
      "5                 6         0.888167       0.00026\n",
      "6                14         0.887435       0.00031\n",
      "7                20         0.886987       0.00036\n",
      "8                18         0.887179       0.00041\n",
      "9                19         0.887165       0.00046\n",
      "10               16         0.887314       0.00051\n",
      "11               17         0.887305       0.00056\n",
      "12               10         0.887595       0.00061\n",
      "13                7         0.887771       0.00066\n",
      "14                8         0.887749       0.00071\n",
      "15                9         0.887694       0.00076\n",
      "16               11         0.887594       0.00081\n",
      "17               12         0.887540       0.00086\n",
      "18               13         0.887504       0.00091\n",
      "19               15         0.887373       0.00096\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Best result: \n",
      "{'epsilon': 1e-05, 'max_iter': 10000}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "pg_svrlin = {'epsilon' : np.arange(start=0.00001, stop=0.001, step=0.00005), 'max_iter':[10000]}\n",
    "linear_svm = LinearSVR()\n",
    "grids_linear_svm = GridSearchCV(linear_svm, pg_svrlin, scoring='r2', cv=tscv, return_train_score=True)\n",
    "grids_linear_svm.fit(X_train,y_train)\n",
    "results_grids_linear_svm = pd.DataFrame(grids_linear_svm.cv_results_)\n",
    "print(results_grids_linear_svm[['rank_test_score','mean_test_score','param_epsilon']])\n",
    "print(\"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"\\n\")\n",
    "print('Best result: ')\n",
    "print(grids_linear_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.8744413376382348\n",
      "0.4088860085762984\n"
     ]
    }
   ],
   "source": [
    "linear_svm_bestmodel = LinearSVR(epsilon=1e-05, max_iter=10000)\n",
    "linear_svm_bestmodel.fit(X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(linear_svm_bestmodel.score(X_train,y_train))\n",
    "print(linear_svm_bestmodel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectations for this  model were higher than actually realized, we are still doing worse than \"expected\" in forecasting, but the linear SVM is among the most performing models for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear SVM\n",
    "\n",
    "At the core of SVM, we have the non-linear vector machine, which relies on the kernel trick to estimate complex non-linear relationships. By the initial model theorized by Sharpe, we might expect these SVM models to be more 'complex' than needed for our task(thus leading to overfitting). Thus I have decided to limit the analysis on this  family of models and make grid search that includes both polynomial and radial basis kernel. From the polynomila grid search I have excluded 1 as degree as we would come back to a linear SVM.\n",
    "\n",
    "NB: here some of the models do not converge as well, but no worries, just scroll to the end of the output for  the  usual table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rank_test_score  mean_test_score param_epsilon param_C param_degree  \\\n",
      "0                 29        -2.074619           0.1     0.1            2   \n",
      "1                164        -2.164993           0.1     0.1            2   \n",
      "2                 28        -0.579165             0     0.1            2   \n",
      "3                  1         0.477985             0     0.1            2   \n",
      "4                110        -2.155404             1     0.1            2   \n",
      "..               ...              ...           ...     ...          ...   \n",
      "283               19         0.385166             0      10           10   \n",
      "284              110        -2.155404             1      10           10   \n",
      "285              110        -2.155404             1      10           10   \n",
      "286               56        -2.155404            10      10           10   \n",
      "287               56        -2.155404            10      10           10   \n",
      "\n",
      "    param_kernel  \n",
      "0           poly  \n",
      "1            rbf  \n",
      "2           poly  \n",
      "3            rbf  \n",
      "4           poly  \n",
      "..           ...  \n",
      "283          rbf  \n",
      "284         poly  \n",
      "285          rbf  \n",
      "286         poly  \n",
      "287          rbf  \n",
      "\n",
      "[288 rows x 6 columns]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Best result: \n",
      "{'C': 0.1, 'degree': 2, 'epsilon': 0, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 10000}\n"
     ]
    }
   ],
   "source": [
    "# now we try non-linear svr\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pg_svm_nonlinear = {'C' : [0.1, 0, 1, 10], 'kernel':['poly', 'rbf'], 'gamma':['scale'],\n",
    "             'epsilon' : [0.1, 0, 1, 10], 'degree' : np.arange(start=2, stop=11, step=1),\n",
    "             'max_iter':[10000]}\n",
    "svm_nonlinear = SVR()\n",
    "grids_svm_nonlinear = GridSearchCV(svm_nonlinear, pg_svm_nonlinear, scoring='r2', cv=tscv, return_train_score=True)\n",
    "grids_svm_nonlinear.fit(X_train,y_train)\n",
    "results_svm_nonlinear = pd.DataFrame(grids_svm_nonlinear.cv_results_)\n",
    "print(results_svm_nonlinear[['rank_test_score','mean_test_score','param_epsilon','param_C', 'param_degree', 'param_kernel']])\n",
    "print(\"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"\\n\")\n",
    "print('Best result: ')\n",
    "print(grids_svm_nonlinear.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.9890536023832558\n",
      "0.6586468312598115\n"
     ]
    }
   ],
   "source": [
    "nonlinear_svm_bestmodel = SVR(epsilon=0, max_iter=10000, degree=2, kernel='rbf', gamma='scale')\n",
    "nonlinear_svm_bestmodel.fit(X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(nonlinear_svm_bestmodel.score(X_train,y_train))\n",
    "print(nonlinear_svm_bestmodel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly as expected the best model, which is a radial basis kernel with epsilon=0 and C=0.1, is highly overfitting as we can see from the score difference between test and training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Regression\n",
    "\n",
    "After SVM, we come back to a non-functional model, which is not the ideal model for a regression task as  our. Nevertheless it is interesting to test the performance of such models for task that are apparently out-of-context.\n",
    "This is a simple tree model and thus consists of only 1 tree with different hyperparameters which are f fine-tuned below.\n",
    "\n",
    "NB: some models might not converge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rank_test_score  mean_test_score param_max_depth param_min_samples_split  \\\n",
      "0                130         0.321663               1                       2   \n",
      "1                143         0.049788               1                       3   \n",
      "2                144         0.026931               1                       2   \n",
      "3                120         0.349674               1                       3   \n",
      "4                141         0.188090               1                       2   \n",
      "..               ...              ...             ...                     ...   \n",
      "139               44         0.605019              20                       3   \n",
      "140               39         0.613530              20                       2   \n",
      "141               38         0.615183              20                       3   \n",
      "142               62         0.587431              20                       2   \n",
      "143               71         0.566562              20                       3   \n",
      "\n",
      "    param_max_features param_min_samples_leaf  \n",
      "0                    1                      1  \n",
      "1                    1                      1  \n",
      "2                    1                      2  \n",
      "3                    1                      2  \n",
      "4                    1                      3  \n",
      "..                 ...                    ...  \n",
      "139                 11                      1  \n",
      "140                 11                      2  \n",
      "141                 11                      2  \n",
      "142                 11                      3  \n",
      "143                 11                      3  \n",
      "\n",
      "[144 rows x 6 columns]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Best result: \n",
      "{'max_depth': 5, 'max_features': 11, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# now we try a simple tree regression\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "pg_tree = {'max_depth' : [1, 2, 3, 5, 10, 20], 'min_samples_split':[2, 3], \n",
    "           'min_samples_leaf':[1, 2, 3], 'max_features' : [1, 3, 5, 11]}\n",
    "simple_tree = DecisionTreeRegressor()\n",
    "grids_simple_tree = GridSearchCV(simple_tree, pg_tree, scoring='r2', cv=tscv, return_train_score=True)\n",
    "grids_simple_tree.fit(X_train,y_train)\n",
    "results_simple_tree = pd.DataFrame(grids_simple_tree.cv_results_)\n",
    "print(results_simple_tree[['rank_test_score','mean_test_score','param_max_depth','param_min_samples_split', 'param_max_features', 'param_min_samples_leaf']])\n",
    "print(\"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"\\n\")\n",
    "print('Best result: ')\n",
    "print(grids_simple_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.9519624362493069\n",
      "0.4073367674054942\n"
     ]
    }
   ],
   "source": [
    "best_simple_tree = DecisionTreeRegressor(max_depth=5, max_features=11, min_samples_leaf=1, \n",
    "                                         min_samples_split=2)\n",
    "best_simple_tree.fit(X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(best_simple_tree.score(X_train,y_train))\n",
    "print(best_simple_tree.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple tree is overfitting a lot and provides really bad results out of sample, as we could have expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "We are now in the world of ensemble models and more complex trees. As always we do have a search gird to fine tune the required hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rank_test_score  mean_test_score param_max_depth param_min_samples_split  \\\n",
      "0                325         0.470812               1                       2   \n",
      "1                328         0.469121               1                       2   \n",
      "2                345         0.461201               1                       2   \n",
      "3                333         0.466979               1                       2   \n",
      "4                329         0.468476               1                       2   \n",
      "..               ...              ...             ...                     ...   \n",
      "499              305         0.631339            None                       5   \n",
      "500              394              NaN            None                       5   \n",
      "501              299         0.633859            None                       5   \n",
      "502              380              NaN            None                       5   \n",
      "503              303         0.632628            None                       5   \n",
      "\n",
      "    param_n_estimators param_min_samples_leaf  \n",
      "0                  100                      1  \n",
      "1                  100                      1  \n",
      "2                  200                      1  \n",
      "3                  200                      1  \n",
      "4                  300                      1  \n",
      "..                 ...                    ...  \n",
      "499                100                      2  \n",
      "500                200                      2  \n",
      "501                200                      2  \n",
      "502                300                      2  \n",
      "503                300                      2  \n",
      "\n",
      "[504 rows x 6 columns]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Best result: \n",
      "{'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100, 'oob_score': True}\n"
     ]
    }
   ],
   "source": [
    "# now we try a Random Forest regression with and without bootstrap and out of bag evaluation\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pg_randomforest = {'max_depth' : [1, 2, 3, 5, 10, 20, None], 'min_samples_split':[2, 3, 5], \n",
    "           'min_samples_leaf':[1, 2], 'n_estimators' : [100, 200, 300], 'bootstrap':[True, False], 'oob_score':[True, False]}\n",
    "randomforest = RandomForestRegressor()\n",
    "grids_randomforest = GridSearchCV(randomforest, pg_randomforest, scoring='r2', cv=tscv, return_train_score=True)\n",
    "grids_randomforest.fit(X_train,y_train)\n",
    "results_randomforest = pd.DataFrame(grids_randomforest.cv_results_)\n",
    "print(results_randomforest[['rank_test_score','mean_test_score','param_max_depth','param_min_samples_split', 'param_n_estimators', 'param_min_samples_leaf']])\n",
    "print(\"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"\\n\")\n",
    "print('Best result: ')\n",
    "print(grids_randomforest.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.9745305406707453\n",
      "0.3195389785005893\n"
     ]
    }
   ],
   "source": [
    "best_randomforest = RandomForestRegressor(max_depth=10, min_samples_leaf=1, n_estimators=300,\n",
    "                                          bootstrap=True)\n",
    "best_randomforest.fit(X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(best_randomforest.score(X_train,y_train))\n",
    "print(best_randomforest.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model do actually overfit a lot, but provides decent results, in the order of the results achieved with simpler but functional model.\n",
    "It is interesting to note that the performance in the training sample is really high, this might be due to the fact that the model is not that  good in forecasting but can provide a good fit for describing historical performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Similarly to the random forest here we test the gradient boosting model, which shares some of the limitations of other tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rank_test_score  mean_test_score param_max_depth param_learning_rate  \\\n",
      "0                140        -0.035216               1                0.01   \n",
      "1                135        -0.019618               1                0.01   \n",
      "2                132        -0.004628               1                0.01   \n",
      "3                120         0.030784               1                0.01   \n",
      "4                139        -0.030604               2                0.01   \n",
      "..               ...              ...             ...                 ...   \n",
      "135               39         0.660168               5                   1   \n",
      "136               28         0.670145            None                   1   \n",
      "137               35         0.661978            None                   1   \n",
      "138               29         0.669161            None                   1   \n",
      "139               34         0.663492            None                   1   \n",
      "\n",
      "    param_n_estimators  \n",
      "0                    1  \n",
      "1                    3  \n",
      "2                    5  \n",
      "3                   10  \n",
      "4                    1  \n",
      "..                 ...  \n",
      "135                 10  \n",
      "136                  1  \n",
      "137                  3  \n",
      "138                  5  \n",
      "139                 10  \n",
      "\n",
      "[140 rows x 5 columns]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Best result: \n",
      "{'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# now we try gradient boosting \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "boostRegress = GradientBoostingRegressor()\n",
    "pg_boostRegress = {'max_depth' : [1, 2, 3, 5, None], 'n_estimators' : [1, 3, 5, 10], \n",
    "                   'learning_rate':[0.01, 0.05, 0.1, 0.5, 0.7, 0.9, 1]}\n",
    "grids_boostRegress = GridSearchCV(boostRegress, pg_boostRegress, scoring='r2', cv=tscv, return_train_score=True)\n",
    "grids_boostRegress.fit(X_train,y_train)\n",
    "results_boostRegress = pd.DataFrame(grids_boostRegress.cv_results_)\n",
    "print(results_boostRegress[['rank_test_score','mean_test_score','param_max_depth','param_learning_rate', 'param_n_estimators']])\n",
    "print(\"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"\\n\")\n",
    "print('Best result: ')\n",
    "print(grids_boostRegress.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.9546029277222555\n",
      "0.512567399710302\n"
     ]
    }
   ],
   "source": [
    "best_boostreg = GradientBoostingRegressor(learning_rate=0.5, max_depth=3, n_estimators=5)\n",
    "best_boostreg.fit(X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(best_boostreg.score(X_train,y_train))\n",
    "print(best_boostreg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with unsupervised learning\n",
    "\n",
    "This is a rather particular approach and I don't expect obtaining great results. The idea is to use K-means clustering over all funds and divide them in possible groups. Once the model for clustering will be ready, we will reshape X_train (which is the BlackRock fund we have used initially) into distances from the centroids of the K-means clustering.\n",
    "Then we will fit a linear svm to the reshaped X in a similar manner to what we have done before.\n",
    "\n",
    "I do not expect this to perform good, it's just to show how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of Gradient boosting is really similar to Random Forest (as we could have expected from lecture notes), but overall performs worst in the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of clusters: 3\n",
      "Inertia: 0.037082923597718254\n",
      "\n",
      "Number of clusters: 5\n",
      "Inertia: 0.021483819714294302\n",
      "\n",
      "Number of clusters: 10\n",
      "Inertia: 0.014527284572224103\n",
      "\n",
      "Number of clusters: 15\n",
      "Inertia: 0.011505576649663266\n",
      "\n",
      "Number of clusters: 20\n",
      "Inertia: 0.010314076735945268\n",
      "\n",
      "Number of clusters: 30\n",
      "Inertia: 0.008216844117609793\n"
     ]
    }
   ],
   "source": [
    "# I will try to cluster the various funds\n",
    "\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "n_clusters = [3, 5, 10, 15, 20, 30]\n",
    "\n",
    "for i, n_cluster in enumerate(n_clusters): \n",
    "    kmcluster = TimeSeriesKMeans(metric=\"dtw\", max_iter=50, n_clusters=n_cluster)\n",
    "    kmcluster.fit(return_funds)\n",
    "    print(f'\\nNumber of clusters: {n_cluster}')\n",
    "    print(f'Inertia: {kmcluster.inertia_}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 320, 1)\n"
     ]
    }
   ],
   "source": [
    "print(kmcluster.cluster_centers_.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        EONIA  MSCI World/Consumer Discrionary  MSCI World/Consumer Staples  \\\n",
      "90  -0.000072                        -0.001187                    -0.007761   \n",
      "256 -0.000091                        -0.045958                    -0.035880   \n",
      "98  -0.000054                        -0.002886                    -0.014025   \n",
      "40  -0.000054                        -0.024131                    -0.036076   \n",
      "19  -0.000054                         0.030045                     0.009819   \n",
      "..        ...                              ...                          ...   \n",
      "129 -0.000072                         0.009735                     0.015800   \n",
      "125 -0.000072                         0.025368                     0.027115   \n",
      "91  -0.000072                         0.002843                    -0.004987   \n",
      "84  -0.000072                         0.016190                     0.014056   \n",
      "7   -0.000036                         0.027181                     0.027445   \n",
      "\n",
      "     MSCI World/Energy  MSCI World/Financials  MSCI World/Health Care  \\\n",
      "90            0.007542              -0.000260                0.004813   \n",
      "256          -0.046455               0.000000               -0.041954   \n",
      "98           -0.006770               0.011101                0.009679   \n",
      "40           -0.001655              -0.013893               -0.018826   \n",
      "19            0.033469               0.054637                0.019508   \n",
      "..                 ...                    ...                     ...   \n",
      "129          -0.043627              -0.003820                0.005205   \n",
      "125           0.024473               0.014659                0.031090   \n",
      "91           -0.000080              -0.009306                0.014533   \n",
      "84            0.024883               0.006822                0.011682   \n",
      "7             0.038431               0.001440                0.023897   \n",
      "\n",
      "     MSCI World/Industrials  MSCI World/Information Tech  \\\n",
      "90                -0.000836                     0.003804   \n",
      "256               -0.048394                    -0.055245   \n",
      "98                 0.006715                     0.007829   \n",
      "40                -0.023490                    -0.028062   \n",
      "19                 0.032506                     0.021859   \n",
      "..                      ...                          ...   \n",
      "129                0.007468                     0.020066   \n",
      "125               -0.001471                     0.014000   \n",
      "91                 0.003891                     0.011264   \n",
      "84                 0.017345                     0.035787   \n",
      "7                  0.011322                     0.032380   \n",
      "\n",
      "     MSCI World/Materials  MSCI World/Telecom svc  MSCI World/Utilities  \n",
      "90               0.014391                0.004411              0.002881  \n",
      "256             -0.038335               -0.024790             -0.032669  \n",
      "98               0.000309               -0.001250              0.005523  \n",
      "40              -0.022910               -0.018739             -0.018925  \n",
      "19               0.060917                0.020177              0.016701  \n",
      "..                    ...                     ...                   ...  \n",
      "129             -0.010088                0.008714              0.017283  \n",
      "125              0.000246                0.029260              0.039781  \n",
      "91               0.007632               -0.014799             -0.010182  \n",
      "84               0.033968                0.008767              0.012312  \n",
      "7                0.015784                0.029477              0.015210  \n",
      "\n",
      "[208 rows x 11 columns]\n",
      "        EONIA  MSCI World/Consumer Discrionary  MSCI World/Consumer Staples  \\\n",
      "26  -0.000054                         0.004586                     0.011734   \n",
      "142 -0.000072                        -0.000658                    -0.023932   \n",
      "11  -0.000053                         0.065606                     0.034265   \n",
      "25  -0.000071                         0.023960                     0.021508   \n",
      "145 -0.000072                         0.009100                     0.002707   \n",
      "165 -0.000072                        -0.004959                     0.018246   \n",
      "207 -0.000090                        -0.009373                    -0.006304   \n",
      "143 -0.000072                         0.009182                    -0.008729   \n",
      "139 -0.000072                        -0.000140                     0.009080   \n",
      "206 -0.000090                         0.000124                     0.006434   \n",
      "185 -0.000072                         0.013082                     0.000850   \n",
      "93  -0.000072                         0.020210                     0.008760   \n",
      "78  -0.000072                         0.018920                     0.012244   \n",
      "88  -0.000072                        -0.003093                     0.002974   \n",
      "60  -0.000072                         0.006059                    -0.002098   \n",
      "259 -0.000091                         0.015188                    -0.013629   \n",
      "43  -0.000071                         0.002836                     0.003259   \n",
      "29  -0.000054                        -0.008544                     0.013789   \n",
      "61  -0.000072                        -0.011343                     0.003171   \n",
      "242 -0.000091                        -0.009405                    -0.002661   \n",
      "211 -0.000090                         0.015852                     0.019005   \n",
      "102 -0.000072                        -0.005623                    -0.007619   \n",
      "224 -0.000091                        -0.072668                    -0.027684   \n",
      "124 -0.000072                         0.011412                    -0.028041   \n",
      "47  -0.000071                        -0.014135                    -0.003675   \n",
      "34  -0.000071                         0.006474                    -0.009395   \n",
      "27  -0.000071                        -0.027206                    -0.015753   \n",
      "70  -0.000072                        -0.004473                     0.006733   \n",
      "44  -0.000054                        -0.001095                    -0.020063   \n",
      "208 -0.000090                         0.018923                     0.017117   \n",
      "173 -0.000072                         0.018795                     0.017458   \n",
      "96  -0.000072                         0.020710                     0.007657   \n",
      "13  -0.000053                         0.037662                     0.026053   \n",
      "157 -0.000072                        -0.032042                    -0.021020   \n",
      "158 -0.000072                        -0.004194                    -0.000279   \n",
      "122 -0.000072                         0.006982                     0.014616   \n",
      "65  -0.000054                         0.004451                    -0.000474   \n",
      "216 -0.000090                        -0.009531                     0.003100   \n",
      "234 -0.000091                         0.015513                     0.014205   \n",
      "24  -0.000071                         0.008554                    -0.004387   \n",
      "3   -0.000053                         0.003628                     0.014645   \n",
      "147 -0.000072                         0.014952                     0.003243   \n",
      "4   -0.000036                         0.003705                    -0.000262   \n",
      "184 -0.000072                         0.016832                     0.001261   \n",
      "120 -0.000072                        -0.042455                    -0.036092   \n",
      "109 -0.000072                         0.025189                    -0.000695   \n",
      "151 -0.000072                        -0.021541                    -0.001641   \n",
      "55  -0.000072                        -0.000208                     0.003309   \n",
      "181 -0.000072                        -0.019990                    -0.003225   \n",
      "64  -0.000072                         0.006762                     0.015038   \n",
      "169 -0.000072                         0.000879                    -0.006124   \n",
      "154 -0.000072                        -0.023536                    -0.014634   \n",
      "\n",
      "     MSCI World/Energy  MSCI World/Financials  MSCI World/Health Care  \\\n",
      "26           -0.006009              -0.009513                0.015634   \n",
      "142           0.013724              -0.011620               -0.004864   \n",
      "11            0.044870               0.051388                0.044699   \n",
      "25            0.020373               0.034868                0.027604   \n",
      "145           0.015729              -0.001403                0.006670   \n",
      "165           0.017559              -0.015022                0.009261   \n",
      "207          -0.012605              -0.002704                0.005718   \n",
      "143          -0.012989              -0.004582                0.002898   \n",
      "139          -0.010233              -0.002810                0.014563   \n",
      "206          -0.003458              -0.004286                0.019167   \n",
      "185           0.039831               0.010555                0.022006   \n",
      "93            0.036896               0.033243                0.011099   \n",
      "78           -0.019812              -0.001174                0.019983   \n",
      "88           -0.015910              -0.017520               -0.003721   \n",
      "60           -0.008944               0.018003               -0.001002   \n",
      "259           0.050417               0.000000               -0.025324   \n",
      "43            0.043945              -0.005113               -0.011774   \n",
      "29            0.012753              -0.010463                0.010591   \n",
      "61           -0.013753              -0.003703                0.013583   \n",
      "242          -0.005082              -0.008927               -0.025901   \n",
      "211           0.031138               0.012966                0.028652   \n",
      "102          -0.043899              -0.012106               -0.009319   \n",
      "224          -0.141372              -0.118817               -0.064355   \n",
      "124           0.024903               0.010545                0.001271   \n",
      "47           -0.015322               0.004098               -0.030962   \n",
      "34           -0.030240               0.002575                0.001121   \n",
      "27            0.002223              -0.032656               -0.025698   \n",
      "70            0.014509              -0.007700                0.003289   \n",
      "44            0.009592               0.015180               -0.004321   \n",
      "208          -0.008678               0.011841                0.020463   \n",
      "173           0.006473               0.011254                0.015871   \n",
      "96            0.000517               0.017033                0.019739   \n",
      "13            0.066334               0.059117                0.006716   \n",
      "157          -0.022633              -0.054675               -0.043629   \n",
      "158          -0.017889              -0.016894               -0.005650   \n",
      "122           0.021927               0.000000               -0.002771   \n",
      "65            0.015438               0.017771                0.017704   \n",
      "216          -0.026705              -0.008861               -0.008012   \n",
      "234          -0.005961               0.047457                0.009586   \n",
      "24            0.022656               0.025751                0.016058   \n",
      "3             0.039213               0.013183                0.012922   \n",
      "147           0.027231              -0.015311                0.023279   \n",
      "4            -0.019262              -0.000756                0.001938   \n",
      "184          -0.009457              -0.001523                0.004017   \n",
      "120          -0.010309              -0.053089               -0.057786   \n",
      "109           0.029967               0.015405                0.025565   \n",
      "151          -0.054567              -0.035227               -0.037569   \n",
      "55           -0.001344              -0.000449                0.000415   \n",
      "181          -0.032653              -0.003282                0.010897   \n",
      "64           -0.008594              -0.003616                0.014592   \n",
      "169           0.000903               0.008477                0.003814   \n",
      "154          -0.016986              -0.015265               -0.013732   \n",
      "\n",
      "     MSCI World/Industrials  MSCI World/Information Tech  \\\n",
      "26                 0.004912                    -0.001744   \n",
      "142               -0.008039                     0.002865   \n",
      "11                 0.063891                     0.061506   \n",
      "25                 0.021400                     0.038520   \n",
      "145                0.012229                     0.011352   \n",
      "165                0.006322                     0.000000   \n",
      "207               -0.010235                    -0.007979   \n",
      "143                0.002107                     0.014243   \n",
      "139               -0.006426                     0.010897   \n",
      "206                0.007513                     0.013533   \n",
      "185                0.021389                     0.028420   \n",
      "93                 0.021540                     0.022765   \n",
      "78                 0.017719                     0.013757   \n",
      "88                -0.004185                    -0.001254   \n",
      "60                 0.009548                     0.018280   \n",
      "259                0.012304                    -0.003947   \n",
      "43                 0.006579                     0.012502   \n",
      "29                -0.002320                    -0.003192   \n",
      "61                -0.011328                    -0.002635   \n",
      "242               -0.013747                    -0.026445   \n",
      "211                0.007465                     0.023258   \n",
      "102               -0.022720                    -0.012567   \n",
      "224               -0.100657                    -0.108133   \n",
      "124                0.019745                    -0.000093   \n",
      "47                -0.006115                    -0.005780   \n",
      "34                 0.002344                     0.007137   \n",
      "27                -0.016420                    -0.018343   \n",
      "70                 0.005910                    -0.000777   \n",
      "44                -0.009485                     0.000304   \n",
      "208                0.010484                     0.023458   \n",
      "173                0.022062                     0.014554   \n",
      "96                 0.012232                     0.022691   \n",
      "13                 0.039377                     0.033860   \n",
      "157               -0.049912                    -0.047876   \n",
      "158               -0.005829                     0.005804   \n",
      "122               -0.006152                    -0.015140   \n",
      "65                 0.010389                     0.007293   \n",
      "216                0.001745                     0.011000   \n",
      "234                0.036752                    -0.002891   \n",
      "24                 0.013121                     0.027189   \n",
      "3                  0.012564                     0.011945   \n",
      "147                0.001553                     0.025251   \n",
      "4                  0.003231                    -0.000659   \n",
      "184               -0.000980                    -0.001001   \n",
      "120               -0.042600                    -0.075847   \n",
      "109                0.024866                     0.037228   \n",
      "151               -0.035707                    -0.022336   \n",
      "55                 0.006018                     0.004059   \n",
      "181               -0.014960                    -0.028074   \n",
      "64                 0.003712                     0.011088   \n",
      "169               -0.004835                     0.003196   \n",
      "154               -0.008005                    -0.024253   \n",
      "\n",
      "     MSCI World/Materials  MSCI World/Telecom svc  MSCI World/Utilities  \n",
      "26               0.012045                0.002949              0.015595  \n",
      "142              0.001877               -0.020114             -0.023822  \n",
      "11               0.057474                0.046187              0.034073  \n",
      "25               0.018129                0.021946              0.019082  \n",
      "145              0.007349                0.023256             -0.001701  \n",
      "165              0.010454                0.007038              0.006655  \n",
      "207             -0.014763               -0.005607             -0.002632  \n",
      "143             -0.002357               -0.020336             -0.009463  \n",
      "139             -0.006026                0.009965              0.000499  \n",
      "206             -0.000055                0.010783              0.009731  \n",
      "185              0.014362                0.018669              0.008056  \n",
      "93               0.012935                0.020333             -0.001679  \n",
      "78               0.011775                0.012074              0.013112  \n",
      "88              -0.005906               -0.006331              0.004801  \n",
      "60               0.026491               -0.017599             -0.010057  \n",
      "259              0.007750               -0.005813             -0.022369  \n",
      "43               0.012537               -0.007840             -0.016417  \n",
      "29               0.001265                0.012231             -0.001143  \n",
      "61              -0.015510               -0.018092             -0.000160  \n",
      "242             -0.001646               -0.023811             -0.013827  \n",
      "211              0.013227                0.029036              0.030746  \n",
      "102             -0.021749               -0.006676             -0.018453  \n",
      "224             -0.053866               -0.073230             -0.089925  \n",
      "124              0.020938                0.007318              0.007775  \n",
      "47              -0.004035               -0.001412              0.001355  \n",
      "34               0.013023               -0.002470             -0.009998  \n",
      "27              -0.014609               -0.016909             -0.005731  \n",
      "70               0.008308               -0.001946              0.007725  \n",
      "44              -0.012087               -0.032528             -0.042835  \n",
      "208              0.012285                0.014330              0.011246  \n",
      "173              0.017204               -0.001855             -0.001758  \n",
      "96               0.021123                0.001804              0.006822  \n",
      "13               0.066262                0.029395              0.025230  \n",
      "157             -0.032426               -0.036752              0.010869  \n",
      "158             -0.001282                0.007515              0.018188  \n",
      "122              0.002152                0.014686              0.008433  \n",
      "65               0.004224               -0.006552              0.006764  \n",
      "216             -0.009376               -0.004266              0.027664  \n",
      "234              0.018989               -0.007886              0.037227  \n",
      "24               0.012836               -0.000847             -0.005786  \n",
      "3                0.030722                0.013088              0.009999  \n",
      "147             -0.007055                0.004762              0.006155  \n",
      "4               -0.007558                0.002218              0.004284  \n",
      "184              0.013990                0.008543              0.005125  \n",
      "120             -0.040549               -0.044029             -0.021046  \n",
      "109              0.030714               -0.000284             -0.009439  \n",
      "151             -0.035297               -0.040494             -0.011424  \n",
      "55              -0.001886                0.016138              0.006001  \n",
      "181             -0.018742               -0.011187              0.013537  \n",
      "64              -0.008093                0.023058              0.029476  \n",
      "169             -0.013106               -0.002809             -0.005483  \n",
      "154             -0.006681                0.014888              0.000674  \n"
     ]
    }
   ],
   "source": [
    "# Since there are 11 indexes I will try to cluster fund indexes \n",
    "km11cluster = TimeSeriesKMeans(metric=\"dtw\", max_iter=20, n_clusters=11)\n",
    "km11cluster.fit(return_funds)\n",
    "cluster_X_train = km11cluster.transform(X_train)\n",
    "cluster_X_test = km11cluster.transform(X_test)\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank_test_score  mean_test_score param_epsilon\n",
      "0                17         0.690663         1e-05\n",
      "1                20         0.689189         6e-05\n",
      "2                19         0.689619       0.00011\n",
      "3                14         0.691632       0.00016\n",
      "4                15         0.690712       0.00021\n",
      "5                18         0.689919       0.00026\n",
      "6                11         0.694371       0.00031\n",
      "7                10         0.694383       0.00036\n",
      "8                 7         0.696171       0.00041\n",
      "9                 2         0.698372       0.00046\n",
      "10                8         0.695804       0.00051\n",
      "11                6         0.696795       0.00056\n",
      "12                5         0.697570       0.00061\n",
      "13                9         0.695332       0.00066\n",
      "14               16         0.690703       0.00071\n",
      "15               12         0.694259       0.00076\n",
      "16               13         0.692911       0.00081\n",
      "17                3         0.698314       0.00086\n",
      "18                4         0.697633       0.00091\n",
      "19                1         0.699873       0.00096\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Best result: \n",
      "{'epsilon': 0.00096, 'max_iter': 10000}\n"
     ]
    }
   ],
   "source": [
    "pg_lsvm_cluster = {'epsilon' : np.arange(start=0.00001, stop=0.001, step=0.00005), 'max_iter':[10000]}\n",
    "lsvm_cluster = LinearSVR()\n",
    "grids_lsvm_cluster = GridSearchCV(linear_svm, pg_svrlin, scoring='r2', cv=tscv, return_train_score=True)\n",
    "grids_lsvm_cluster.fit(cluster_X_train, y_train)\n",
    "results_grids_lsvm_cluster = pd.DataFrame(grids_lsvm_cluster.cv_results_)\n",
    "print(results_grids_lsvm_cluster[['rank_test_score','mean_test_score','param_epsilon']])\n",
    "print(\"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"\\n\")\n",
    "print('Best result: ')\n",
    "print(grids_lsvm_cluster.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.7880485937737601\n",
      "0.13203888714997125\n"
     ]
    }
   ],
   "source": [
    "lvsm_cluster_bestmodel = LinearSVR(epsilon=0.00096, max_iter=10000)\n",
    "lvsm_cluster_bestmodel.fit(cluster_X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(lvsm_cluster_bestmodel.score(cluster_X_train,y_train))\n",
    "print(lvsm_cluster_bestmodel.score(cluster_X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the result is not really good but it might an interesting approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "This is a very simple approach to Neural Networks, we will perform a simple GridSearch over some possible shapes and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank_test_score  mean_test_score      param_hidden_layer_sizes  \\\n",
      "0                36        -0.022965                     [5, 5, 5]   \n",
      "1                42        -0.023150                  [10, 20, 10]   \n",
      "2                48        -0.023508                  [50, 50, 50]   \n",
      "3                30        -0.022815                [100, 20, 100]   \n",
      "4                35        -0.022937  [10, 10, 10, 10, 10, 10, 10]   \n",
      "5                39        -0.023026                     [5, 5, 5]   \n",
      "6                37        -0.023002                  [10, 20, 10]   \n",
      "7                45        -0.023404                  [50, 50, 50]   \n",
      "8                33        -0.022928                [100, 20, 100]   \n",
      "9                28        -0.022736  [10, 10, 10, 10, 10, 10, 10]   \n",
      "10               50        -0.023750                     [5, 5, 5]   \n",
      "11               49        -0.023725                  [10, 20, 10]   \n",
      "12               43        -0.023158                  [50, 50, 50]   \n",
      "13               17        -0.021940                [100, 20, 100]   \n",
      "14               34        -0.022931  [10, 10, 10, 10, 10, 10, 10]   \n",
      "15               38        -0.023007                     [5, 5, 5]   \n",
      "16               31        -0.022826                  [10, 20, 10]   \n",
      "17               21        -0.022270                  [50, 50, 50]   \n",
      "18               47        -0.023485                [100, 20, 100]   \n",
      "19               19        -0.022165  [10, 10, 10, 10, 10, 10, 10]   \n",
      "20               46        -0.023456                     [5, 5, 5]   \n",
      "21               27        -0.022610                  [10, 20, 10]   \n",
      "22               41        -0.023095                  [50, 50, 50]   \n",
      "23               23        -0.022473                [100, 20, 100]   \n",
      "24               29        -0.022758  [10, 10, 10, 10, 10, 10, 10]   \n",
      "25               13         0.495576                     [5, 5, 5]   \n",
      "26                6         0.784617                  [10, 20, 10]   \n",
      "27                8         0.777067                  [50, 50, 50]   \n",
      "28                5         0.793056                [100, 20, 100]   \n",
      "29               12         0.525701  [10, 10, 10, 10, 10, 10, 10]   \n",
      "30               10         0.608337                     [5, 5, 5]   \n",
      "31                3         0.804219                  [10, 20, 10]   \n",
      "32                7         0.779492                  [50, 50, 50]   \n",
      "33                4         0.796728                [100, 20, 100]   \n",
      "34               15         0.316762  [10, 10, 10, 10, 10, 10, 10]   \n",
      "35               14         0.492822                     [5, 5, 5]   \n",
      "36                2         0.811231                  [10, 20, 10]   \n",
      "37                9         0.632993                  [50, 50, 50]   \n",
      "38                1         0.818308                [100, 20, 100]   \n",
      "39               11         0.533456  [10, 10, 10, 10, 10, 10, 10]   \n",
      "40               20        -0.022194                     [5, 5, 5]   \n",
      "41               24        -0.022523                  [10, 20, 10]   \n",
      "42               18        -0.022141                  [50, 50, 50]   \n",
      "43               16        -0.019404                [100, 20, 100]   \n",
      "44               22        -0.022454  [10, 10, 10, 10, 10, 10, 10]   \n",
      "45               40        -0.023081                     [5, 5, 5]   \n",
      "46               32        -0.022886                  [10, 20, 10]   \n",
      "47               44        -0.023340                  [50, 50, 50]   \n",
      "48               25        -0.022525                [100, 20, 100]   \n",
      "49               26        -0.022542  [10, 10, 10, 10, 10, 10, 10]   \n",
      "\n",
      "   param_activation param_alpha  \n",
      "0          logistic      0.0001  \n",
      "1          logistic      0.0001  \n",
      "2          logistic      0.0001  \n",
      "3          logistic      0.0001  \n",
      "4          logistic      0.0001  \n",
      "5          logistic       0.001  \n",
      "6          logistic       0.001  \n",
      "7          logistic       0.001  \n",
      "8          logistic       0.001  \n",
      "9          logistic       0.001  \n",
      "10         logistic        0.01  \n",
      "11         logistic        0.01  \n",
      "12         logistic        0.01  \n",
      "13         logistic        0.01  \n",
      "14         logistic        0.01  \n",
      "15         logistic         0.1  \n",
      "16         logistic         0.1  \n",
      "17         logistic         0.1  \n",
      "18         logistic         0.1  \n",
      "19         logistic         0.1  \n",
      "20         logistic           1  \n",
      "21         logistic           1  \n",
      "22         logistic           1  \n",
      "23         logistic           1  \n",
      "24         logistic           1  \n",
      "25             relu      0.0001  \n",
      "26             relu      0.0001  \n",
      "27             relu      0.0001  \n",
      "28             relu      0.0001  \n",
      "29             relu      0.0001  \n",
      "30             relu       0.001  \n",
      "31             relu       0.001  \n",
      "32             relu       0.001  \n",
      "33             relu       0.001  \n",
      "34             relu       0.001  \n",
      "35             relu        0.01  \n",
      "36             relu        0.01  \n",
      "37             relu        0.01  \n",
      "38             relu        0.01  \n",
      "39             relu        0.01  \n",
      "40             relu         0.1  \n",
      "41             relu         0.1  \n",
      "42             relu         0.1  \n",
      "43             relu         0.1  \n",
      "44             relu         0.1  \n",
      "45             relu           1  \n",
      "46             relu           1  \n",
      "47             relu           1  \n",
      "48             relu           1  \n",
      "49             relu           1  \n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Best result: \n",
      "{'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': [100, 20, 100], 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# To close this exercise we will try a Neural Network\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "hidden_layer_sizes = [[5,5,5], [10,20,10], [50,50,50], [100, 20, 100], [10,10,10,10,10,10,10]]\n",
    "pg_mlpr = {\"hidden_layer_sizes\": hidden_layer_sizes, \"activation\": [\"logistic\", \"relu\"], \"solver\": [\"lbfgs\"], \"alpha\": [0.0001,0.001,0.01,0.1,1]}\n",
    "mlpr = MLPRegressor(max_iter=7000)\n",
    "grids_mlpr = GridSearchCV(estimator=mlpr, param_grid=pg_mlpr)\n",
    "grids_mlpr.fit(X_train,y_train)\n",
    "results_mlpr = pd.DataFrame(grids_mlpr.cv_results_)\n",
    "print(results_mlpr[['rank_test_score','mean_test_score','param_hidden_layer_sizes','param_activation', 'param_alpha']])\n",
    "print(\"\\n\")\n",
    "print(\"-\"*100)\n",
    "print(\"\\n\")\n",
    "print('Best result: ')\n",
    "print(grids_mlpr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test R-squared:\n",
      "0.8764305422957386\n",
      "0.33765328970068276\n"
     ]
    }
   ],
   "source": [
    "mlpr_bestmodel = MLPRegressor(max_iter=7000, activation='relu', alpha=0.01,\n",
    "                              hidden_layer_sizes=[100, 20, 100], solver='lbfgs')\n",
    "mlpr_bestmodel.fit(X_train,y_train)\n",
    "print(\"Train/test R-squared:\")\n",
    "print(mlpr_bestmodel.score(X_train,y_train))\n",
    "print(mlpr_bestmodel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the Neural Network does not perform good, but this more than expected as our training sample is way too small for an effective training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Overall none of the model we have tested do perform good enough. This is due to some evident limitations in our data, we do not have enough data to train effectively most of the more advanced model we have tested (SVM, Random Forest, NN).\n",
    "\n",
    "Furthermore, the task itself does not seems to fit some of the models we have tested, such as KNN, and trees in general. Thus is not surprising that the best results are obtained with simpler linear models, which should actually fit better the theoretical background of Style Analysis itself.\n",
    "\n",
    "One last interesting point is the good performance in training for Random Forest, this makes me think that a better result can be obtained if we could increase our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
